{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping multiple pages "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have practiced web scraping when all the information we wanted was on a single table of a site. What happens when we want to scrape information from multiple pages?\n",
    "\n",
    "## First example - IMDB \n",
    "\n",
    "Go to https://www.imdb.com/search/title/ and enter the following parameters, leaving all other fields blank or with its default value:\n",
    "\n",
    "- Title Type: Feature film\n",
    "\n",
    "- Release date: From 1990 to 1992\n",
    "\n",
    "- User Rating: 7.5 to \"-\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The page you get should be familiar. There's a list with movies and each movie has its title, release year, crew, etc. You could inspect the page and build the code to collect the date.\n",
    "\n",
    "Note the resulting query obtained contain hundreds of movies, and each page only contains 50 of them (you can change the settings to obtain up to 250 movies/page, but that still won't be the complete list).\n",
    "\n",
    "One way to automatize multi page web scraping is to look at the URLs. \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
    "\n",
    "Note what the url looks like if you scroll down and click on \"Next\", the URL is now: \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
    "\n",
    "Can you see the pattern?\n",
    "\n",
    "our search options are in the parameters title_type, release_date and user_rating. Then, we have the start parameter, which jumps in intervals of 50, and the ref_ parameter, which takes the value of \"adv_nxt\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#  import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#  url: this time, start with the 'second' page\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# download html with a request, check response code \n",
    "response = requests.get(url)\n",
    "response.status_code"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#  parse html (create the 'soup')\n",
    "soup_imdb = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# check that the html code looks as expected \n",
    "#soup_imdb.prettify())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we'll have to build a list of values which jumps by 50, up to the total number of movies we want to scrape.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# define iterations \n",
    "iter = range(1, 537, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# check the iterations work\n",
    "iter"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "range(1, 537, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# create the url string for the page search, populate with the iterations\n",
    "for i in iter:\n",
    "    start_at = str(i)\n",
    "    url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=' + start_at + '&ref_=adv_nxt'\n",
    "    print(url)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=1&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=151&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=201&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=251&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=301&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=351&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=401&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=451&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=501&ref_=adv_nxt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test the urls \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Respectful scraping:\n",
    "\n",
    "Before starting with the actual scraping, though, there's something we need to note when sending automated requests to websites: it's good practice to let a few seconds pass in between requests. \n",
    "\n",
    "Some pages don't like being scraped and will block your IP if they detect you are sending automated requests. Others might have a small server for the traffic they handle, and sending too many requests might crash the site.\n",
    "\n",
    "The sleep module will help us with that. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from time import sleep\n",
    "\n",
    "#simple example \n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    sleep(3)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# To make it more \"human\", we can randomize the waiting time:\n",
    "from random import randint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    wait_time = randint(1, 4)\n",
    "    print('sleep time', wait_time)\n",
    "    sleep(wait_time)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "sleep time 4\n",
      "1\n",
      "sleep time 4\n",
      "2\n",
      "sleep time 2\n",
      "3\n",
      "sleep time 1\n",
      "4\n",
      "sleep time 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assembling the script to send and store multiple requests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "pages = []\n",
    "for i in iter:\n",
    "    start_at = str(i)\n",
    "    url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=' + start_at + '&ref_=adv_nxt'\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code)\n",
    "    pages.append(response)\n",
    "    wait_time = randint(1, 4)\n",
    "    sleep(wait_time)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pages"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#BeautifulSoup(pages[2].content, 'html.parser')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: if you print the object pages after running the code above, you'll just see the response code messages, but the html code is still accessible and you can parse it the same way as before"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build code to collect the relevant information from the Request \n",
    "\n",
    "this is what we need : \n",
    "\n",
    "##### Parse just the first page, for testing purposes\n",
    "- soup=BeautifulSoup(pages[0].content, \"html.parser\")\n",
    "\n",
    "##### title and synopsis\n",
    "\n",
    "- soup.select(\"div.lister-item-content > h3 > a\")\n",
    "- soup.select(\"div.lister-item-content > p:nth-child(4)\")"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### titles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Parse just the first page, for testing purposes\n",
    "soup = BeautifulSoup(pages[0].content, 'html.parser')\n",
    "soup.select('h3 > a')\n",
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "\n",
    "# Trim the selection\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<a href=\"/title/tt0103064/\">Terminator 2: Tag der Abrechnung</a>,\n",
       " <a href=\"/title/tt0099685/\">GoodFellas - Drei Jahrzehnte in der Mafia</a>,\n",
       " <a href=\"/title/tt0099674/\">Der Pate 3</a>,\n",
       " <a href=\"/title/tt0105236/\">Reservoir Dogs: Wilde Hunde</a>,\n",
       " <a href=\"/title/tt0102926/\">Das Schweigen der Lämmer</a>,\n",
       " <a href=\"/title/tt0104257/\">Eine Frage der Ehre</a>,\n",
       " <a href=\"/title/tt0104691/\">Der letzte Mohikaner</a>,\n",
       " <a href=\"/title/tt0100802/\">Total Recall - Die totale Erinnerung</a>,\n",
       " <a href=\"/title/tt0101507/\">Boyz n the Hood - Jungs im Viertel</a>,\n",
       " <a href=\"/title/tt0105695/\">Erbarmungslos</a>,\n",
       " <a href=\"/title/tt0099785/\">Kevin - Allein zu Haus</a>,\n",
       " <a href=\"/title/tt0104952/\">Mein Vetter Winnie</a>,\n",
       " <a href=\"/title/tt0099348/\">Der mit dem Wolf tanzt</a>,\n",
       " <a href=\"/title/tt0103074/\">Thelma &amp; Louise</a>,\n",
       " <a href=\"/title/tt0105323/\">Der Duft der Frauen</a>,\n",
       " <a href=\"/title/tt0099810/\">Jagd auf Roter Oktober</a>,\n",
       " <a href=\"/title/tt0099487/\">Edward mit den Scherenhänden</a>,\n",
       " <a href=\"/title/tt0103639/\">Aladdin</a>,\n",
       " <a href=\"/title/tt0101414/\">Die Schöne und das Biest</a>,\n",
       " <a href=\"/title/tt0100157/\">Misery</a>,\n",
       " <a href=\"/title/tt0101921/\">Grüne Tomaten</a>,\n",
       " <a href=\"/title/tt0102138/\">JFK: Tatort Dallas</a>,\n",
       " <a href=\"/title/tt0101889/\">König der Fischer</a>,\n",
       " <a href=\"/title/tt0099871/\">Jacob's Ladder - In der Gewalt des Jenseits</a>,\n",
       " <a href=\"/title/tt0099077/\">Zeit des Erwachens</a>,\n",
       " <a href=\"/title/tt0101428/\">Die schöne Querulantin</a>,\n",
       " <a href=\"/title/tt0105046/\">Von Mäusen und Menschen</a>,\n",
       " <a href=\"/title/tt0103873/\">Braindead</a>,\n",
       " <a href=\"/title/tt0106308/\">Armee der Finsternis</a>,\n",
       " <a href=\"/title/tt0104348/\">Glengarry Glen Ross</a>,\n",
       " <a href=\"/title/tt0104797/\">Malcolm X</a>,\n",
       " <a href=\"/title/tt0105151/\">The Player</a>,\n",
       " <a href=\"/title/tt0103939/\">Chaplin - Das Leben der unsterblichen Filmlegende</a>,\n",
       " <a href=\"/title/tt0100150/\">Miller's Crossing</a>,\n",
       " <a href=\"/title/tt0101410/\">Barton Fink</a>,\n",
       " <a href=\"/title/tt0103905/\">Mann beißt Hund</a>,\n",
       " <a href=\"/title/tt0101605/\">Die Commitments</a>,\n",
       " <a href=\"/title/tt0101700/\">Delicatessen</a>,\n",
       " <a href=\"/title/tt0101258/\">Days of Being Wild</a>,\n",
       " <a href=\"/title/tt0104652/\">Porco Rosso</a>,\n",
       " <a href=\"/title/tt0101318/\">Die Liebenden von Pont-Neuf</a>,\n",
       " <a href=\"/title/tt0101765/\">Die zwei Leben der Veronika</a>,\n",
       " <a href=\"/title/tt0102536/\">Night on Earth</a>,\n",
       " <a href=\"/title/tt0101985/\">Ein Sommer zum Verlieben</a>,\n",
       " <a href=\"/title/tt0104684/\">Hard Boiled</a>,\n",
       " <a href=\"/title/tt0100234/\">Close Up</a>,\n",
       " <a href=\"/title/tt0102587/\">Tränen der Erinnerung</a>,\n",
       " <a href=\"/title/tt0100998/\">Akira Kurosawa's Träume</a>,\n",
       " <a href=\"/title/tt0101640/\">Rote Laterne</a>,\n",
       " <a href=\"/title/tt0105017/\">Noises Off! - Der nackte Wahnsinn</a>]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### synopsis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select('p:nth-child(4)')[0].get_text().strip()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A cyborg, identical to the one who failed to kill Sarah Connor, must now protect her ten year old son, John Connor, from a more advanced and powerful cyborg.'"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### combine all the code \n",
    "\n",
    "There are many approaches to do this. The one we'll follow is: \n",
    "\n",
    "- Loop through the pages we collected, parse them (\"create the soup\") and store the parsed pages in a list. \n",
    "\n",
    "- For each parsed page, select the \"blocks of HTML elements\" that contain all the information of each movie (the title, the synopsis and other stuff). \n",
    "\n",
    "- For each one of the \"blocks\" we collected in the previous step: \n",
    "\n",
    "    - Get the movie titles and store them in a list \n",
    "\n",
    "    - Get the synopsis and store them in a list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "titles = []\n",
    "synopsis = []\n",
    "pages_parsed = []\n",
    "for i in range(len(pages)):\n",
    "    pages_parsed.append(BeautifulSoup(pages[i].content, 'html.parser'))\n",
    "    movies_html = pages_parsed[i].select('div.lister-item-content')\n",
    "    for j in range(len(movies_html)):\n",
    "        titles.append(movies_html[j].select('h3 > a')[0].get_text())\n",
    "        synopsis.append(movies_html[j].select('p:nth-child(4)')[0].get_text().strip())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# check the output and identify any wrangling steps we missed\n",
    "len(titles), len(synopsis)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(537, 537)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "#titles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "#synopsis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2nd example - Scraping presidents\n",
    "\n",
    "Our objective is to create a dataframe with information about the presidents of the United States. To do this, we will go through 5 steps:\n",
    "\n",
    "1. Scrape this [list of presidents of the United States](https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# 1. import libraries\n",
    "\n",
    "# 2. find url and store it in a variable\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States'\n",
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# 4.2. check that the html code looks like it should\n",
    "#soup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Collect all the links to the Wikipedia page of each president.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "presidents = []\n",
    "for i in range(84):\n",
    "    presidents += soup.select('tbody > tr:nth-child(' + str(i) + ') > td:nth-child(4) > b > a')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "presidents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/George_Washington\" title=\"George Washington\">George Washington</a>,\n",
       " <a href=\"/wiki/John_Adams\" title=\"John Adams\">John Adams</a>,\n",
       " <a href=\"/wiki/Thomas_Jefferson\" title=\"Thomas Jefferson\">Thomas Jefferson</a>,\n",
       " <a href=\"/wiki/James_Madison\" title=\"James Madison\">James Madison</a>,\n",
       " <a href=\"/wiki/James_Monroe\" title=\"James Monroe\">James Monroe</a>,\n",
       " <a href=\"/wiki/John_Quincy_Adams\" title=\"John Quincy Adams\">John Quincy Adams</a>,\n",
       " <a href=\"/wiki/Andrew_Jackson\" title=\"Andrew Jackson\">Andrew Jackson</a>,\n",
       " <a href=\"/wiki/Martin_Van_Buren\" title=\"Martin Van Buren\">Martin Van Buren</a>,\n",
       " <a href=\"/wiki/William_Henry_Harrison\" title=\"William Henry Harrison\">William Henry Harrison</a>,\n",
       " <a href=\"/wiki/John_Tyler\" title=\"John Tyler\">John Tyler</a>,\n",
       " <a href=\"/wiki/James_K._Polk\" title=\"James K. Polk\">James K. Polk</a>,\n",
       " <a href=\"/wiki/Zachary_Taylor\" title=\"Zachary Taylor\">Zachary Taylor</a>,\n",
       " <a href=\"/wiki/Millard_Fillmore\" title=\"Millard Fillmore\">Millard Fillmore</a>,\n",
       " <a href=\"/wiki/Franklin_Pierce\" title=\"Franklin Pierce\">Franklin Pierce</a>,\n",
       " <a href=\"/wiki/James_Buchanan\" title=\"James Buchanan\">James Buchanan</a>,\n",
       " <a href=\"/wiki/Abraham_Lincoln\" title=\"Abraham Lincoln\">Abraham Lincoln</a>,\n",
       " <a href=\"/wiki/Andrew_Johnson\" title=\"Andrew Johnson\">Andrew Johnson</a>,\n",
       " <a href=\"/wiki/Ulysses_S._Grant\" title=\"Ulysses S. Grant\">Ulysses S. Grant</a>,\n",
       " <a href=\"/wiki/Rutherford_B._Hayes\" title=\"Rutherford B. Hayes\">Rutherford B. Hayes</a>,\n",
       " <a href=\"/wiki/James_A._Garfield\" title=\"James A. Garfield\">James A. Garfield</a>,\n",
       " <a href=\"/wiki/Chester_A._Arthur\" title=\"Chester A. Arthur\">Chester A. Arthur</a>,\n",
       " <a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>,\n",
       " <a href=\"/wiki/Benjamin_Harrison\" title=\"Benjamin Harrison\">Benjamin Harrison</a>,\n",
       " <a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>,\n",
       " <a href=\"/wiki/William_McKinley\" title=\"William McKinley\">William McKinley</a>,\n",
       " <a href=\"/wiki/Theodore_Roosevelt\" title=\"Theodore Roosevelt\">Theodore Roosevelt</a>,\n",
       " <a href=\"/wiki/William_Howard_Taft\" title=\"William Howard Taft\">William Howard Taft</a>,\n",
       " <a href=\"/wiki/Woodrow_Wilson\" title=\"Woodrow Wilson\">Woodrow Wilson</a>,\n",
       " <a href=\"/wiki/Warren_G._Harding\" title=\"Warren G. Harding\">Warren G. Harding</a>,\n",
       " <a href=\"/wiki/Calvin_Coolidge\" title=\"Calvin Coolidge\">Calvin Coolidge</a>,\n",
       " <a href=\"/wiki/Herbert_Hoover\" title=\"Herbert Hoover\">Herbert Hoover</a>,\n",
       " <a href=\"/wiki/Franklin_D._Roosevelt\" title=\"Franklin D. Roosevelt\">Franklin D. Roosevelt</a>,\n",
       " <a href=\"/wiki/Harry_S._Truman\" title=\"Harry S. Truman\">Harry S. Truman</a>,\n",
       " <a href=\"/wiki/Dwight_D._Eisenhower\" title=\"Dwight D. Eisenhower\">Dwight D. Eisenhower</a>,\n",
       " <a href=\"/wiki/John_F._Kennedy\" title=\"John F. Kennedy\">John F. Kennedy</a>,\n",
       " <a href=\"/wiki/Lyndon_B._Johnson\" title=\"Lyndon B. Johnson\">Lyndon B. Johnson</a>,\n",
       " <a href=\"/wiki/Richard_Nixon\" title=\"Richard Nixon\">Richard Nixon</a>,\n",
       " <a href=\"/wiki/Gerald_Ford\" title=\"Gerald Ford\">Gerald Ford</a>,\n",
       " <a href=\"/wiki/Jimmy_Carter\" title=\"Jimmy Carter\">Jimmy Carter</a>,\n",
       " <a href=\"/wiki/Ronald_Reagan\" title=\"Ronald Reagan\">Ronald Reagan</a>,\n",
       " <a href=\"/wiki/George_H._W._Bush\" title=\"George H. W. Bush\">George H. W. Bush</a>,\n",
       " <a href=\"/wiki/Bill_Clinton\" title=\"Bill Clinton\">Bill Clinton</a>,\n",
       " <a href=\"/wiki/George_W._Bush\" title=\"George W. Bush\">George W. Bush</a>,\n",
       " <a href=\"/wiki/Barack_Obama\" title=\"Barack Obama\">Barack Obama</a>,\n",
       " <a href=\"/wiki/Donald_Trump\" title=\"Donald Trump\">Donald Trump</a>,\n",
       " <a href=\"/wiki/Joe_Biden\" title=\"Joe Biden\">Joe Biden</a>]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# we can access the links searching for the attribute \"href\"\n",
    "# in each element\n",
    "presidents[40]['href']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/wiki/George_H._W._Bush'"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# Now, we just assemble a new request to the link\n",
    "url = 'https://en.wikipedia.org' + presidents[0]['href']\n",
    "response = requests.get(url)\n",
    "response\n",
    "# send request\n",
    "\n",
    "\n",
    "# parse & store html\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "#soup.find('table', {'class':'infobox vcard'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Scrape the Wikipedia page of each president.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this step we could very well store the whole wikipedia page for each president, or just the tiny, final pieces of information. Storing the boxes is a middle ground (we don't have too much noise but retain the flexibility of deciding later which specific elements to extract).\n",
    "\n",
    "When sending multiple requests, remember to be respectful by spacing the requests a few seconds from each other. We will also ping the success code to monitor that everything is going well:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# 2. find url and store it in a variable\n",
    "pres_soups = []\n",
    "for pres in presidents:\n",
    "    # send request\n",
    "    url = 'https://en.wikipedia.org' + pres['href']\n",
    "    response = requests.get(url)\n",
    "    print(pres.get_text(), response.status_code)\n",
    "    # parse & store html\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pres_soups.append(soup.find('table', {'class':'infobox vcard'}))\n",
    "    # respectful nap:\n",
    "    wait_time = randint(1, 2)\n",
    "    sleep(wait_time)\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "George Washington 200\n",
      "John Adams 200\n",
      "Thomas Jefferson 200\n",
      "James Madison 200\n",
      "James Monroe 200\n",
      "John Quincy Adams 200\n",
      "Andrew Jackson 200\n",
      "Martin Van Buren 200\n",
      "William Henry Harrison 200\n",
      "John Tyler 200\n",
      "James K. Polk 200\n",
      "Zachary Taylor 200\n",
      "Millard Fillmore 200\n",
      "Franklin Pierce 200\n",
      "James Buchanan 200\n",
      "Abraham Lincoln 200\n",
      "Andrew Johnson 200\n",
      "Ulysses S. Grant 200\n",
      "Rutherford B. Hayes 200\n",
      "James A. Garfield 200\n",
      "Chester A. Arthur 200\n",
      "Grover Cleveland 200\n",
      "Benjamin Harrison 200\n",
      "Grover Cleveland 200\n",
      "William McKinley 200\n",
      "Theodore Roosevelt 200\n",
      "William Howard Taft 200\n",
      "Woodrow Wilson 200\n",
      "Warren G. Harding 200\n",
      "Calvin Coolidge 200\n",
      "Herbert Hoover 200\n",
      "Franklin D. Roosevelt 200\n",
      "Harry S. Truman 200\n",
      "Dwight D. Eisenhower 200\n",
      "John F. Kennedy 200\n",
      "Lyndon B. Johnson 200\n",
      "Richard Nixon 200\n",
      "Gerald Ford 200\n",
      "Jimmy Carter 200\n",
      "Ronald Reagan 200\n",
      "George H. W. Bush 200\n",
      "Bill Clinton 200\n",
      "George W. Bush 200\n",
      "Barack Obama 200\n",
      "Donald Trump 200\n",
      "Joe Biden 200\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Find and store information about each president.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We extracted the 'infoboxes': now it's time to extract specific information from them. First test what can we get from a single president and then assemble a loop for all of them.\n",
    "\n",
    "Here, we will use [the string argument](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-string-argument) in the find function, since wikipedia tags and classes are not always helpful to locate. The string argument allows us to locate elements by its actual content."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "#Birthday\n",
    "pres_soups[40].find('span', {'class':'bday'}).get_text()\n",
    "#Political party\n",
    "pres_soups[12].find('th', string = 'Political party').parent.find('a').get_text()\n",
    "#Number of sons/daughters\n",
    "pres_soups[12].find('th', string = 'Children').parent.find_all('li')\n",
    "# collect with a loop \n",
    "name=[]\n",
    "dob=[]\n",
    "party=[]\n",
    "children=[]\n",
    "\n",
    "for presi in pres_soups:\n",
    "    name.append(presi.find(\"div\",{\"class\":\"fn\"}).get_text())\n",
    "    dob.append(presi.find(\"span\",{\"class\":\"bday\"}).get_text())\n",
    "    party.append(presi.find(\"th\",string=\"Political party\").parent.find(\"a\").get_text())\n",
    "    try:\n",
    "        children.append(len(presi.find(\"th\",string=\"Children\").parent.find_all(\"li\")))\n",
    "    except:\n",
    "        children.append(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Organize the information in a dataframe where we have each president as a row and each variable we collected as a column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "pres_df = pd.DataFrame({'name':name, 'dob':dob, 'party':party, 'children':children})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "pres_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                name         dob                  party  children\n",
       "0  George Washington  1732-02-22            Independent         0\n",
       "1         John Adams  1735-10-30     Pro-Administration         0\n",
       "2   Thomas Jefferson  1743-04-13  Democratic-Republican         6\n",
       "3      James Madison  1751-03-16  Democratic-Republican         0\n",
       "4       James Monroe  1758-04-28  Democratic-Republican         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dob</th>\n",
       "      <th>party</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>1732-02-22</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Adams</td>\n",
       "      <td>1735-10-30</td>\n",
       "      <td>Pro-Administration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>1743-04-13</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James Madison</td>\n",
       "      <td>1751-03-16</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James Monroe</td>\n",
       "      <td>1758-04-28</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}